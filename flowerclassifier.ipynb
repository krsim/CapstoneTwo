{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Models\n\n","metadata":{}},{"cell_type":"markdown","source":"\n# Table of Contents\n1. [Introduction](#introduction)\n2. [Import Libraries](#libraries)\n3. [Distribution Strategy](#strategy)\n4. [Load Data](#load)\n5. [Some Helper Functions](#functions)\n6. [Get Datasets Ready For Training](#datasets)\n7. [Define and Train Models](#models)\n    1. [VGG](#vgg)\n    2. [Resnet50](#resnet)\n    3. [InceptionNet V3](#inception)\n8. [Evaluate Models](#evaluate)\n    1. [Training Curves](#curves)\n    2. [Confusion Matrix](#matrix)\n        1. [VGG](#vggmatrix)\n        2. [Resnet50](#resnetmatrix)\n        3. [InceptionNet V3](#inceptionmatrix)","metadata":{}},{"cell_type":"markdown","source":"\n## Introduction <a name=introduction>\n    \n    In this notebook, the data is uploaded from Kaggle dataset and stored in a Google Coud Storage bucket to be used with TPUs. The data itself is in a tfrec format for convenient distribution to the TPU cores.\n\nThe final layers of three pretrained networks are trained on the data and the results are compared in the next notebook. ","metadata":{}},{"cell_type":"markdown","source":"\n## Import libraries <a name=\"libraries\">","metadata":{}},{"cell_type":"code","source":"import math, re, os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nprint(\"Tensorflow version \" + tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T11:13:14.376844Z","iopub.execute_input":"2022-02-25T11:13:14.377207Z","iopub.status.idle":"2022-02-25T11:13:19.717529Z","shell.execute_reply.started":"2022-02-25T11:13:14.377114Z","shell.execute_reply":"2022-02-25T11:13:19.716072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Distribution Strategy <a name=\"strategy\">","metadata":{}},{"cell_type":"code","source":"# Detect TPU, return distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(\"Running on TPU \", tpu.master())\nexcept ValueError:\n    tpu = None\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T11:13:19.719125Z","iopub.execute_input":"2022-02-25T11:13:19.719458Z","iopub.status.idle":"2022-02-25T11:13:25.684659Z","shell.execute_reply.started":"2022-02-25T11:13:19.719426Z","shell.execute_reply":"2022-02-25T11:13:25.683568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Load Data <a name=\"load\">","metadata":{}},{"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\nprint(GCS_DS_PATH) ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T11:13:25.686000Z","iopub.execute_input":"2022-02-25T11:13:25.686329Z","iopub.status.idle":"2022-02-25T11:13:26.092543Z","shell.execute_reply.started":"2022-02-25T11:13:25.686300Z","shell.execute_reply":"2022-02-25T11:13:26.091575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ndirs = os.listdir(\"../input\")\n\nfor file in dirs:\n    print(file)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T11:13:26.093804Z","iopub.execute_input":"2022-02-25T11:13:26.094049Z","iopub.status.idle":"2022-02-25T11:13:26.099533Z","shell.execute_reply.started":"2022-02-25T11:13:26.094015Z","shell.execute_reply":"2022-02-25T11:13:26.098854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Kaggle data is split into a labeled training set, a labeled validation set, and an unlabeled test set. The unlabeled set is used for competition submission and will not be used here. Instead, the validation set will be used as the test set and the training set will be split for validation.","metadata":{}},{"cell_type":"code","source":"\nIMAGE_SIZE = [331, 331]\nGCS_PATH =  GCS_DS_PATH + '/tfrecords-jpeg-331x331'\nAUTO = tf.data.experimental.AUTOTUNE\n\nvalidation_split = 0.25\ndata_files = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\nsplit = len(data_files) - int(len(data_files) * validation_split)\n\nTRAINING_FILENAMES = data_files[:split]\nVALIDATION_FILENAMES = data_files[split:]\nTEST_FILENAMES =  tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\n\nCLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 102\n\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T11:13:26.101677Z","iopub.execute_input":"2022-02-25T11:13:26.102170Z","iopub.status.idle":"2022-02-25T11:13:26.275687Z","shell.execute_reply.started":"2022-02-25T11:13:26.102140Z","shell.execute_reply":"2022-02-25T11:13:26.274903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Helper Functions <a name=\"functions\">\n\nFunctios to augment and retrieve data","metadata":{}},{"cell_type":"code","source":"def zoom(x: tf.Tensor) -> tf.Tensor:\n    #Zoom Augmentation\n    \n    scales = list(np.arange(0.8, 1.2, 0.02))\n    boxes = np.zeros((len(scales), 4))\n    \n    for i, scale in enumerate(scales):\n        x1 = y1 = 0.5 - (0.5 * scale)\n        x2 = y2 = 0.5 + (0.5 * scale)\n        boxes[i] = [x1, y1, x2, y2]\n        \n    def random_crop(img):\n        #Create different crops\n        crops = tf.image.crop_and_resize([img], boxes=boxes, box_indices=np.zeros(len(scales)), crop_size=(331, 331) )\n        return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n    \n    choice =  tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n    \n    #Applies 50% of the time\n    return tf.cond(choice < 0.5, lambda: x, lambda: random_crop(x))\n\n\ndef rotate(x: tf.Tensor) -> tf.Tensor:\n    \n    image = tf.cond(tf.greater(tf.random.uniform([]), 0.5), lambda: tf.image.rot90(x), lambda: x)\n    return image\n\ndef data_augment(image, label):\n    #Execute a series of random image augmentations \n    \n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_hue(image, 0.08)\n    image = tf.image.random_saturation(image, 0.6, 1.6)\n    image = tf.image.random_brightness(image, 0.05)\n    image = tf.image.random_contrast(image, 0.7, 1.3)\n    image = zoom(image)\n    image = rotate(image)\n    return image, label\n\nREPEAT_DATA_AUGMENT = 12\n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES)\n    dataset = dataset.repeat(REPEAT_DATA_AUGMENT)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\n# Learning Rate Schedule for Fine Tuning #\ndef exponential_lr(epoch,\n                   start_lr = 0.00001, min_lr = 0.00001, max_lr = 0.00005,\n                   rampup_epochs = 5, sustain_epochs = 0,\n                   exp_decay = 0.8):\n\n    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n        # linear increase from start to rampup_epochs\n        if epoch < rampup_epochs:\n            lr = ((max_lr - start_lr) /\n                  rampup_epochs * epoch + start_lr)\n        # constant max_lr during sustain_epochs\n        elif epoch < rampup_epochs + sustain_epochs:\n            lr = max_lr\n        # exponential decay towards min_lr\n        else:\n            lr = ((max_lr - min_lr) *\n                  exp_decay**(epoch - rampup_epochs - sustain_epochs) +\n                  min_lr)\n        return lr\n    return lr(epoch,\n              start_lr,\n              min_lr,\n              max_lr,\n              rampup_epochs,\n              sustain_epochs,\n              exp_decay)\n\n\n\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES) * REPEAT_DATA_AUGMENT\nNUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nprint('Dataset: {} training images, {} validation images, {} test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))","metadata":{"execution":{"iopub.status.busy":"2022-02-25T11:13:26.277026Z","iopub.execute_input":"2022-02-25T11:13:26.277345Z","iopub.status.idle":"2022-02-25T11:13:26.306384Z","shell.execute_reply.started":"2022-02-25T11:13:26.277306Z","shell.execute_reply":"2022-02-25T11:13:26.305069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Get Datasets Ready for Training <a name=\"datasets\">","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\nds_train = get_training_dataset()\nds_valid = get_validation_dataset()\nds_test = get_test_dataset()\n\nprint(\"Training:\", ds_train)\nprint (\"Validation:\", ds_valid)\nprint(\"Test:\", ds_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T11:13:26.307662Z","iopub.execute_input":"2022-02-25T11:13:26.308254Z","iopub.status.idle":"2022-02-25T11:13:27.032651Z","shell.execute_reply.started":"2022-02-25T11:13:26.308217Z","shell.execute_reply":"2022-02-25T11:13:27.031663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T11:13:27.033831Z","iopub.execute_input":"2022-02-25T11:13:27.034045Z","iopub.status.idle":"2022-02-25T11:13:27.038645Z","shell.execute_reply.started":"2022-02-25T11:13:27.034020Z","shell.execute_reply":"2022-02-25T11:13:27.038002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Models <a name=\"models\">","metadata":{}},{"cell_type":"markdown","source":"### VGG <a name=\"vgg\">","metadata":{}},{"cell_type":"code","source":"\n# Define training epochs\nEPOCHS = 30\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n\nwith strategy.scope():\n    pretrained_vgg_model = tf.keras.applications.vgg16.VGG16(\n        weights='imagenet',\n        include_top=False ,\n        input_shape=[*IMAGE_SIZE, 3]\n    )\n    for layer in pretrained_vgg_model.layers:\n        if layer.name == 'block14_sepconv1' or layer.name == 'block14_sepconv2':\n           layer.trainable = True\n        else:\n            layer.trainable = False\n    \n    vgg_model = tf.keras.Sequential([\n        # To a base pretrained on ImageNet to extract features from images...\n        pretrained_vgg_model,\n        # ... attach a new head to act as a classifier.\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(500, activation='relu'),\n        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T11:13:27.039774Z","iopub.execute_input":"2022-02-25T11:13:27.040132Z","iopub.status.idle":"2022-02-25T11:13:29.882913Z","shell.execute_reply.started":"2022-02-25T11:13:27.040095Z","shell.execute_reply":"2022-02-25T11:13:29.882158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_vgg_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T11:13:29.884154Z","iopub.execute_input":"2022-02-25T11:13:29.884482Z","iopub.status.idle":"2022-02-25T11:13:29.900476Z","shell.execute_reply.started":"2022-02-25T11:13:29.884442Z","shell.execute_reply":"2022-02-25T11:13:29.896416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import optimizers\n\noptimizer = optimizers.Adam(lr=0.01)\n\nvgg_model.compile(\n    optimizer=optimizer,\n    loss = 'sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy'],\n)\n\nvgg_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T11:13:29.901844Z","iopub.execute_input":"2022-02-25T11:13:29.902088Z","iopub.status.idle":"2022-02-25T11:13:29.947690Z","shell.execute_reply.started":"2022-02-25T11:13:29.902061Z","shell.execute_reply":"2022-02-25T11:13:29.947022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n#from tensorflow.keras.callbacks import EarlyStopping\n\n#early_stopping = EarlyStopping(patience=3, verbose=1,restore_best_weights=True)\nlr_callback = tf.keras.callbacks.LearningRateScheduler(exponential_lr, verbose=True)\n\nvgg_history = vgg_model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=EPOCHS,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    callbacks=[lr_callback]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T11:13:29.948904Z","iopub.execute_input":"2022-02-25T11:13:29.952014Z","iopub.status.idle":"2022-02-25T13:52:19.417250Z","shell.execute_reply.started":"2022-02-25T11:13:29.951981Z","shell.execute_reply":"2022-02-25T13:52:19.416251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\nvgg_model.save('./vggmodel', options=save_locally)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T13:52:19.418519Z","iopub.execute_input":"2022-02-25T13:52:19.418769Z","iopub.status.idle":"2022-02-25T13:52:23.179168Z","shell.execute_reply.started":"2022-02-25T13:52:19.418741Z","shell.execute_reply":"2022-02-25T13:52:23.178375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Resnet50 <a name=\"resnet\">","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    pretrained_resnet_model = tf.keras.applications.resnet50.ResNet50(\n        weights='imagenet',\n        include_top=False ,\n        input_shape=[*IMAGE_SIZE, 3]\n    )\n    for layer in pretrained_resnet_model.layers:\n        if layer.name == 'block14_sepconv1' or layer.name == 'block14_sepconv2':\n           layer.trainable = True\n        else:\n            layer.trainable = False\n    \n    resnet_model = tf.keras.Sequential([\n        # To a base pretrained on ImageNet to extract features from images...\n        pretrained_resnet_model,\n        # ... attach a new head to act as a classifier.\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(500, activation='relu'),\n        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T13:52:23.188143Z","iopub.execute_input":"2022-02-25T13:52:23.188457Z","iopub.status.idle":"2022-02-25T13:52:36.446766Z","shell.execute_reply.started":"2022-02-25T13:52:23.188423Z","shell.execute_reply":"2022-02-25T13:52:36.446139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_resnet_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T13:52:36.448379Z","iopub.execute_input":"2022-02-25T13:52:36.448953Z","iopub.status.idle":"2022-02-25T13:52:36.526194Z","shell.execute_reply.started":"2022-02-25T13:52:36.448913Z","shell.execute_reply":"2022-02-25T13:52:36.525320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_model.compile(\n    optimizer=optimizer,\n    loss = 'sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy'],\n)\n\nresnet_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T13:52:36.527811Z","iopub.execute_input":"2022-02-25T13:52:36.528215Z","iopub.status.idle":"2022-02-25T13:52:36.580627Z","shell.execute_reply.started":"2022-02-25T13:52:36.528166Z","shell.execute_reply":"2022-02-25T13:52:36.579767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nresnet_history = resnet_model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=EPOCHS,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    callbacks=[lr_callback]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T13:52:36.582222Z","iopub.execute_input":"2022-02-25T13:52:36.582544Z","iopub.status.idle":"2022-02-25T16:32:40.242964Z","shell.execute_reply.started":"2022-02-25T13:52:36.582506Z","shell.execute_reply":"2022-02-25T16:32:40.241890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\nresnet_model.save('./resnetmodel', options=save_locally)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T16:32:40.244200Z","iopub.execute_input":"2022-02-25T16:32:40.244475Z","iopub.status.idle":"2022-02-25T16:33:15.601405Z","shell.execute_reply.started":"2022-02-25T16:32:40.244443Z","shell.execute_reply":"2022-02-25T16:33:15.600421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### InceptionNet V3 <a name=\"inception\">","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    pretrained_inception_model = tf.keras.applications.inception_v3.InceptionV3(\n        weights='imagenet',\n        include_top=False ,\n        input_shape=[*IMAGE_SIZE, 3]\n    )\n    for layer in pretrained_inception_model.layers:\n        if layer.name == 'block14_sepconv1' or layer.name == 'block14_sepconv2':\n           layer.trainable = True\n        else:\n            layer.trainable = False\n    \n    inception_model = tf.keras.Sequential([\n        # To a base pretrained on ImageNet to extract features from images...\n        pretrained_inception_model,\n        # ... attach a new head to act as a classifier.\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(500, activation='relu'),\n        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T16:33:15.615086Z","iopub.execute_input":"2022-02-25T16:33:15.615430Z","iopub.status.idle":"2022-02-25T16:33:34.181488Z","shell.execute_reply.started":"2022-02-25T16:33:15.615400Z","shell.execute_reply":"2022-02-25T16:33:34.180513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_inception_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T16:33:34.182752Z","iopub.execute_input":"2022-02-25T16:33:34.182979Z","iopub.status.idle":"2022-02-25T16:33:34.325514Z","shell.execute_reply.started":"2022-02-25T16:33:34.182953Z","shell.execute_reply":"2022-02-25T16:33:34.324443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inception_model.compile(\n    optimizer=optimizer,\n    loss = 'sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy'],\n)\n\ninception_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T16:33:34.327057Z","iopub.execute_input":"2022-02-25T16:33:34.328077Z","iopub.status.idle":"2022-02-25T16:33:34.483047Z","shell.execute_reply.started":"2022-02-25T16:33:34.328038Z","shell.execute_reply":"2022-02-25T16:33:34.481999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inception_history = inception_model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=EPOCHS,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    callbacks=[lr_callback]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T16:33:34.484585Z","iopub.execute_input":"2022-02-25T16:33:34.484910Z","iopub.status.idle":"2022-02-25T19:16:33.072605Z","shell.execute_reply.started":"2022-02-25T16:33:34.484869Z","shell.execute_reply":"2022-02-25T19:16:33.071506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\ninception_model.save('./inceptionmodel', options=save_locally)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:17:48.616685Z","iopub.execute_input":"2022-02-25T19:17:48.617460Z","iopub.status.idle":"2022-02-25T19:18:47.674174Z","shell.execute_reply.started":"2022-02-25T19:17:48.617414Z","shell.execute_reply":"2022-02-25T19:18:47.673387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Models <a name=\"evaluate\">","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n\ndef display_confusion_matrix(cmat, score, precision, recall):\n    plt.figure(figsize=(15,15))\n    ax = plt.gca()\n    ax.matshow(cmat, cmap='Reds')\n    ax.set_xticks(range(len(CLASSES)))\n    ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n    ax.set_yticks(range(len(CLASSES)))\n    ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})\n    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    titlestring = \"\"\n    if score is not None:\n        titlestring += 'f1 = {:.3f} '.format(score)\n    if precision is not None:\n        titlestring += '\\nprecision = {:.3f} '.format(precision)\n    if recall is not None:\n        titlestring += '\\nrecall = {:.3f} '.format(recall)\n    if len(titlestring) > 0:\n        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n    plt.show()\n\ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:20:32.475288Z","iopub.execute_input":"2022-02-25T19:20:32.475734Z","iopub.status.idle":"2022-02-25T19:20:33.449458Z","shell.execute_reply.started":"2022-02-25T19:20:32.475700Z","shell.execute_reply":"2022-02-25T19:20:33.448578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Curves <a name=\"curves\">","metadata":{}},{"cell_type":"code","source":"#Training Curves for the vgg model\ndisplay_training_curves(\n    vgg_history.history['loss'],\n    vgg_history.history['val_loss'],\n    'vgg model loss',\n    211)\n\ndisplay_training_curves(\n    vgg_history.history['sparse_categorical_accuracy'],\n    vgg_history.history['val_sparse_categorical_accuracy'],\n    'vgg model accuracy',\n    212)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:20:41.489762Z","iopub.execute_input":"2022-02-25T19:20:41.490133Z","iopub.status.idle":"2022-02-25T19:20:42.078424Z","shell.execute_reply.started":"2022-02-25T19:20:41.490097Z","shell.execute_reply":"2022-02-25T19:20:42.077421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training Curves for the resnet 50 model\ndisplay_training_curves(\n    resnet_history.history['loss'],\n    resnet_history.history['val_loss'],\n    'resnet model loss',\n    211)\n\ndisplay_training_curves(\n    vgg_history.history['sparse_categorical_accuracy'],\n    vgg_history.history['val_sparse_categorical_accuracy'],\n    'resnet model accuracy',\n    212)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:20:52.606596Z","iopub.execute_input":"2022-02-25T19:20:52.607381Z","iopub.status.idle":"2022-02-25T19:20:53.171010Z","shell.execute_reply.started":"2022-02-25T19:20:52.607331Z","shell.execute_reply":"2022-02-25T19:20:53.170133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training Curves for the inception V3 model\ndisplay_training_curves(\n    inception_history.history['loss'],\n    inception_history.history['val_loss'],\n    'inception V3 model loss',\n    211)\n\ndisplay_training_curves(\n    inception_history.history['sparse_categorical_accuracy'],\n    inception_history.history['val_sparse_categorical_accuracy'],\n    'inception V3 model accuracy',\n    212)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:21:00.411895Z","iopub.execute_input":"2022-02-25T19:21:00.412568Z","iopub.status.idle":"2022-02-25T19:21:00.899225Z","shell.execute_reply.started":"2022-02-25T19:21:00.412522Z","shell.execute_reply":"2022-02-25T19:21:00.898258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Confusion Matrix <a name=\"matrix\">","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### VGG <a name=\"vggmatrix\">","metadata":{}},{"cell_type":"code","source":"cmdataset = get_test_dataset(ordered=True)\nimages_ds = cmdataset.map(lambda image, label: image)\nlabels_ds = cmdataset.map(lambda image, label: label).unbatch()\n\ncm_correct_labels = next(iter(labels_ds.batch(NUM_TEST_IMAGES))).numpy()\nvgg_cm_probabilities = vgg_model.predict(images_ds)\nvgg_cm_predictions = np.argmax(vgg_cm_probabilities, axis=-1)\n\nlabels = range(len(CLASSES))\nvgg_cmat = confusion_matrix(\n    cm_correct_labels,\n    vgg_cm_predictions,\n    labels=labels\n)\nvgg_cmat = (vgg_cmat.T / vgg_cmat.sum(axis=1)).T # normalize","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg_f1_score = f1_score(\n    cm_correct_labels,\n    vgg_cm_predictions,\n    labels=labels,\n    average='macro',\n)\nvgg_precision = precision_score(\n    cm_correct_labels,\n    vgg_cm_predictions,\n    labels=labels,\n    average='macro',\n)\nvgg_recall = recall_score(\n    cm_correct_labels,\n    vgg_cm_predictions,\n    labels=labels,\n    average='macro',\n)\ndisplay_confusion_matrix(vgg_cmat, vgg_score, vgg_precision, vgg_recall)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Resnet50 <a name=\"resnetmatrix\">","metadata":{}},{"cell_type":"code","source":"cmdataset = get_test_dataset(ordered=True)\nimages_ds = cmdataset.map(lambda image, label: image)\nlabels_ds = cmdataset.map(lambda image, label: label).unbatch()\n\ncm_correct_labels = next(iter(labels_ds.batch(NUM_TEST_IMAGES))).numpy()\nresnet_cm_probabilities = resnet_model.predict(images_ds)\nresnet_cm_predictions = np.argmax(resnet_cm_probabilities, axis=-1)\n\nlabels = range(len(CLASSES))\nresnet_cmat = confusion_matrix(\n    cm_correct_labels,\n    resnet_cm_predictions,\n    labels=labels,\n)\nresnet_cmat = (resnet_cmat.T / resnet_cmat.sum(axis=1)).T # normalize","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_f1_score = f1_score(\n    cm_correct_labels,\n    resnet_cm_predictions,\n    labels=labels,\n    average='macro',\n)\nresnet_precision = precision_score(\n    cm_correct_labels,\n    resnet_cm_predictions,\n    labels=labels,\n    average='macro',\n)\nresnet_recall = recall_score(\n    cm_correct_labels,\n    resnet_cm_predictions,\n    labels=labels,\n    average='macro',\n)\ndisplay_confusion_matrix(resnet_cmat, resnet_score, resnet_precision, resnet_recall)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### InceptionNet V3 <a name=\"inceptionmatrix\">","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}